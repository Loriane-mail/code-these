{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19087cfb",
   "metadata": {},
   "source": [
    "# Récupération des fichiers .mat dans les dossier r2_D_gamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec43914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Spécifier le chemin du dossier contenant les fichiers .mat\n",
    "directory_path = 'D:/Loriane/Résultats laboratoire/2023-08-10 SPT 4D6 R-HT Jurkat RL-HT/Control/r2_D_gamma'\n",
    "parent_directory = os.path.dirname(directory_path)\n",
    "output_excel_path = os.path.join(parent_directory, 'combined_output.xlsx')\n",
    "\n",
    "# Dictionnaire pour regrouper les données des fichiers\n",
    "data_dict = {}\n",
    "\n",
    "# Fonction pour extraire la partie nécessaire du nom de fichier\n",
    "def extract_base_name(filename):\n",
    "    # Séparer le nom du fichier par \"_\"\n",
    "    parts = filename.split('_')\n",
    "    # Prendre la première partie (ex : \"puit1\")\n",
    "    prefix = parts[0]\n",
    "    # Rechercher la partie contenant \"cell\" avant \"w1Quad-TIRF\"\n",
    "    suffix = next(part for part in parts if 'cell' in part)\n",
    "    # Retourner le nom combiné (par ex : \"puit1_cell1\")\n",
    "    return f\"{prefix}_{suffix}\"\n",
    "\n",
    "# Parcourir tous les fichiers dans le dossier\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('t1.xml_r2_D_gamma.mat'):\n",
    "        # Extraire la bonne partie du nom\n",
    "        base_name = extract_base_name(filename)\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        mat_data = scipy.io.loadmat(file_path)\n",
    "\n",
    "        # Extraire les données\n",
    "        D = mat_data['D'].T.flatten() if 'D' in mat_data else []\n",
    "        gamma = mat_data['gamma'].T.flatten() if 'gamma' in mat_data else []\n",
    "\n",
    "        # Vérifier la présence de Quad-TIRF488, Quad-TIRF642 ou Quad-TIRF561\n",
    "        if 'Quad-TIRF488' in filename or 'Quad-TIRF561' in filename:\n",
    "            key_D = 'D RANKL'\n",
    "            key_gamma = 'gamma RANKL'\n",
    "        elif 'Quad-TIRF642' in filename:\n",
    "            key_D = 'D RANK'\n",
    "            key_gamma = 'gamma RANK'\n",
    "\n",
    "        if base_name not in data_dict:\n",
    "            data_dict[base_name] = {}\n",
    "\n",
    "        # Vérifier si D ou gamma est vide\n",
    "        if len(D) == 0 and len(gamma) == 0:\n",
    "            continue  # Ne rien faire si les deux sont vides\n",
    "\n",
    "        # Ajouter les données au dictionnaire\n",
    "        data_dict[base_name][key_D] = D\n",
    "        data_dict[base_name][key_gamma] = gamma\n",
    "\n",
    "# Créer un writer Excel pour écrire les différentes feuilles\n",
    "with pd.ExcelWriter(output_excel_path) as writer:\n",
    "    for base_name, data in data_dict.items():\n",
    "        # Si data est vide, passer cette feuille\n",
    "        if not data:\n",
    "            continue\n",
    "\n",
    "        # Déterminer la longueur maximale des colonnes\n",
    "        max_length = max(len(data[key]) for key in data.keys())\n",
    "        \n",
    "        # Compléter les colonnes avec des NaN si leur longueur est inférieure à la longueur maximale\n",
    "        for key in data.keys():\n",
    "            if len(data[key]) < max_length:\n",
    "                data[key] = np.pad(data[key], (0, max_length - len(data[key])), constant_values=np.nan)\n",
    "\n",
    "        # Créer le DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Limiter le nom de la feuille à 31 caractères\n",
    "        sheet_name = base_name[:31]\n",
    "        \n",
    "        try:\n",
    "            print(df.head())\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'écriture de la feuille {sheet_name}: {e}\")\n",
    "\n",
    "print(f\"Le fichier Excel a été sauvegardé sous {output_excel_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49265b-37ff-4a75-8d02-d3a15035d97e",
   "metadata": {},
   "source": [
    "Analyses de D de RANK et RANKL par GMM en sans fixer de composante (faite pour 4D6 seul et Jurkat seul pour calculer les valeurs de la composante lente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ba1f8-54c6-490c-937f-5599790b2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bon code pour les valeurs entre 10^-4 et 10^0 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import simps  # Pour l'intégration numérique\n",
    "import os\n",
    "\n",
    "# Charger le fichier Excel\n",
    "file_path = '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/combined_output_2024-07-09_Jurkatseul.xlsx'\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "# Colonnes que nous allons traiter\n",
    "columns = ['Diff RANK', 'Diff RANKL']\n",
    "\n",
    "# Paramètres pour les calculs\n",
    "pixel = 0.11  # en micromètres\n",
    "#trackmate_time = 0.248  # en secondes si 1 couleur : 0.124, si 2 couleurs : 0.248, si 4 couleurs : 0.43\n",
    "time_lag = 0.248 #/ trackmate_time  # en secondes\n",
    "\n",
    "# Parcourir chaque feuille du fichier Excel\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Calculer Diff RANK et Diff RANKL\n",
    "    if 'D RANK' in data.columns:\n",
    "        data['Diff RANK'] = data['D RANK'] * (pixel ** 2) / time_lag\n",
    "    if 'D RANKL' in data.columns:\n",
    "        data['Diff RANKL'] = data['D RANKL'] * (pixel ** 2) / time_lag\n",
    "\n",
    "    # Générer le nom du fichier de sortie pour les paramètres\n",
    "    output_file = f'gmm_parameters_{os.path.basename(file_path)}_{sheet_name}.txt'\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Analyse des composantes GMM pour le fichier : {file_path}, feuille : {sheet_name}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        # Fonction pour appliquer le GMM et tracer les graphes pour les populations fast et slow\n",
    "        def analyze_and_plot(column_name):\n",
    "            column_data = data[column_name].dropna()  # Supprimer les valeurs manquantes\n",
    "\n",
    "            # Appliquer la transformation logarithmique (log10)\n",
    "            log_data = np.log10(column_data[column_data > 0])  # Exclure les valeurs <= 0\n",
    "\n",
    "            # Limiter les données à l'intervalle [-4, 0] (car 10^-4 à 10^0 correspond à cet intervalle sur log10)\n",
    "            log_data = log_data[(log_data >= -4) & (log_data <= 0)]\n",
    "\n",
    "            # Reshape des données pour le GMM\n",
    "            log_data_reshaped = log_data.values.reshape(-1, 1)\n",
    "\n",
    "            # Appliquer le modèle GMM avec deux composantes (fast et slow)\n",
    "            gmm = GaussianMixture(n_components=2, covariance_type='full')\n",
    "            gmm.fit(log_data_reshaped)\n",
    "\n",
    "            # Obtenir les paramètres du GMM\n",
    "            means = gmm.means_.flatten()\n",
    "            covariances = np.sqrt(gmm.covariances_).flatten()  # Extraire les écarts-types\n",
    "            weights = gmm.weights_\n",
    "\n",
    "            # Réordonner les composantes pour garantir que la lente (slow) est toujours avant la rapide (fast)\n",
    "            if means[0] > means[1]:\n",
    "                means = means[::-1]  # Inverser l'ordre des moyennes\n",
    "                covariances = covariances[::-1]  # Inverser les écarts-types\n",
    "                weights = weights[::-1]  # Inverser les poids\n",
    "\n",
    "            # Calculer le seuil qui sépare fast et slow (moyenne des deux moyennes)\n",
    "            threshold = (means[0] + means[1]) / 2\n",
    "\n",
    "            # Prédire les labels (fast/slow) pour chaque point de données en fonction des moyennes réordonnées\n",
    "            labels = np.where(log_data < threshold, 0, 1)  # 0 pour slow, 1 pour fast\n",
    "\n",
    "            # Calculer le pourcentage de chaque sous-population\n",
    "            slow_percentage = np.sum(labels == 0) / len(labels) * 100\n",
    "            fast_percentage = np.sum(labels == 1) / len(labels) * 100\n",
    "\n",
    "            # Enregistrer les paramètres dans le fichier texte\n",
    "            f.write(f\"Colonne analysée : {column_name}\\n\")\n",
    "            f.write(f\"Pourcentage de molécules lentes (slow) : {slow_percentage:.2f}%\\n\")\n",
    "            f.write(f\"Pourcentage de molécules rapides (fast) : {fast_percentage:.2f}%\\n\")\n",
    "            f.write(f\"Moyennes des composantes GMM : {means}\\n\")\n",
    "            f.write(f\"Écarts-types des composantes GMM : {covariances}\\n\")\n",
    "            f.write(f\"Seuil de séparation fast/slow : {threshold}\\n\")\n",
    "\n",
    "            # Tracer un histogramme des données avec les deux composantes du GMM\n",
    "            sns.histplot(log_data, bins=30, kde=False, color='gray', label=f'Données {column_name}', stat='density')\n",
    "\n",
    "            # Tracer la densité de probabilité pour chaque composante du GMM\n",
    "            x_vals = np.linspace(-4, 0, 1000)  # Limiter l'intervalle de traçage entre -4 et 0\n",
    "\n",
    "            # Calculer la densité pour chaque composante\n",
    "            pdf_0 = norm.pdf(x_vals, means[0], covariances[0])\n",
    "            pdf_1 = norm.pdf(x_vals, means[1], covariances[1])\n",
    "\n",
    "            # Plot the individual component PDFs, scaled by the GMM weights\n",
    "            plt.plot(x_vals, weights[0] * pdf_0, label='Composante Lente (Slow)', color='green')\n",
    "            plt.plot(x_vals, weights[1] * pdf_1, label='Composante Rapide (Fast)', color='orange')\n",
    "\n",
    "            # Tracer la densité totale du GMM\n",
    "            pdf_total = weights[0] * pdf_0 + weights[1] * pdf_1  # Densité totale comme somme pondérée\n",
    "            plt.plot(x_vals, pdf_total, label='Ajustement GMM', color='red')\n",
    "\n",
    "            # Marquer le seuil de séparation\n",
    "            plt.axvline(x=threshold, color='blue', linestyle='--', label='Seuil Fast/Slow')\n",
    "\n",
    "            # Ajouter des légendes et titres\n",
    "            plt.legend()\n",
    "            plt.xlabel('log10(Données)')\n",
    "            plt.ylabel('Fréquence')\n",
    "            plt.title(f'Modèle GMM pour {column_name} (Fast vs Slow) - Feuille {sheet_name}')\n",
    "\n",
    "            # Limiter les axes à l'intervalle [-4, 0]\n",
    "            plt.xlim([-4, 0])\n",
    "\n",
    "            # Sauvegarder la figure avec nom complet\n",
    "            figure_name = f'gmm_{column_name}_{os.path.basename(file_path)}_{sheet_name}.png'\n",
    "            plt.savefig(figure_name)\n",
    "            plt.close()  # Fermer la figure pour ne pas l'afficher dans l'environnement actuel\n",
    "\n",
    "            # Calcul de l'air sous la courbe pour chaque composante et l'ajustement total\n",
    "            area_slow = simps(weights[0] * pdf_0, x_vals)\n",
    "            area_fast = simps(weights[1] * pdf_1, x_vals)\n",
    "            area_total = simps(pdf_total, x_vals)\n",
    "\n",
    "            # Enregistrer les aires sous la courbe dans le fichier texte\n",
    "            f.write(f\"Aire sous la courbe de la composante Lente (Slow) : {area_slow:.4f}\\n\")\n",
    "            f.write(f\"Aire sous la courbe de la composante Rapide (Fast) : {area_fast:.4f}\\n\")\n",
    "            f.write(f\"Aire sous la courbe de l'ajustement GMM total : {area_total:.4f}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        # Appliquer l'analyse uniquement sur 'Diff RANK' et 'Diff RANKL'\n",
    "        for col in ['Diff RANK', 'Diff RANKL']:\n",
    "            if col in data.columns:\n",
    "                analyze_and_plot(col)\n",
    "\n",
    "print(f\"Les résultats ont été sauvegardés et les figures ont été enregistrées.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3c69a-9c6a-4de1-b581-dadde2782c54",
   "metadata": {},
   "source": [
    "Analyses de D de RANK et RANKL par GMM en fixant la fraction lente uniquement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c88e0-d8b3-48f6-b474-24eea4687970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code bon pour fixer composante lente uniquement \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import simps  # Pour l'intégration numérique\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Charger le fichier Excel\n",
    "file_path = '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/output Trackmate Jurkat Control Vs Ac/combined_output Control 29 02 2024.xlsx'\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "# Colonnes que nous allons traiter\n",
    "columns = ['Diff RANK', 'Diff RANKL']\n",
    "\n",
    "# Paramètres pour les calculs\n",
    "pixel = 0.11  # en micromètres\n",
    "#trackmate_time = 0.248  # en secondes si 1 couleur : 0.124, si 2 couleurs : 0.248, si 4 couleurs : 0.43\n",
    "time_lag = 0.248  # en secondes\n",
    "\n",
    "# Parcourir chaque feuille du fichier Excel\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Calculer Diff RANK et Diff RANKL\n",
    "    if 'D RANK' in data.columns:\n",
    "        data['Diff RANK'] = data['D RANK'] * (pixel ** 2) / time_lag\n",
    "    if 'D RANKL' in data.columns:\n",
    "        data['Diff RANKL'] = data['D RANKL'] * (pixel ** 2) / time_lag\n",
    "\n",
    "    # Générer le nom du fichier de sortie pour les paramètres\n",
    "    output_file = f'gmm_parameters_{os.path.basename(file_path)}_{sheet_name}.txt'\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Analyse des composantes GMM pour le fichier : {file_path}, feuille : {sheet_name}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        # Fonction pour appliquer le GMM et tracer les graphes pour les populations fast et slow\n",
    "        def analyze_and_plot(column_name):\n",
    "            column_data = data[column_name].dropna()  # Supprimer les valeurs manquantes\n",
    "\n",
    "            # Appliquer la transformation logarithmique (log10)\n",
    "            log_data = np.log10(column_data[column_data > 0])  # Exclure les valeurs <= 0\n",
    "\n",
    "            # Limiter les données à l'intervalle [-4, 0] (car 10^-4 à 10^0 correspond à cet intervalle sur log10)\n",
    "            log_data = log_data[(log_data >= -4) & (log_data <= 0)]\n",
    "\n",
    "            # Reshape des données pour le GMM\n",
    "            log_data_reshaped = log_data.values.reshape(-1, 1)\n",
    "\n",
    "            # Fixer uniquement la première composante pour Diff RANK ou Diff RANKL\n",
    "            if column_name == 'Diff RANK':\n",
    "                fixed_means = np.array([[-2.77749681]])  # Moyenne de la composante lente pour RANK\n",
    "                fixed_covariances = np.array([[[0.492384**2]]])  # Variance de la composante lente pour RANK\n",
    "            elif column_name == 'Diff RANKL':\n",
    "                fixed_means = np.array([[-2.648777307]])  # Moyenne de la composante lente pour RANKL\n",
    "                fixed_covariances = np.array([[[0.455597573**2]]])  # Variance de la composante lente pour RANKL\n",
    "            else:\n",
    "                return\n",
    "\n",
    "            # Initialiser le modèle GMM\n",
    "            gmm = GaussianMixture(n_components=2, covariance_type='full')\n",
    "\n",
    "            # Fixer les paramètres de la première composante (lente)\n",
    "            gmm.means_init = np.vstack([fixed_means, np.mean(log_data_reshaped, axis=0)])  # Moyenne de la deuxième composante à estimer\n",
    "            gmm.fit(log_data_reshaped)\n",
    "\n",
    "            # Pour garantir que la première composante reste fixée, on remplace après l'ajustement\n",
    "            gmm.means_[0] = fixed_means\n",
    "            gmm.covariances_[0] = fixed_covariances\n",
    "            gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(gmm.covariances_))\n",
    "\n",
    "            # Extraire les moyennes et covariances finales\n",
    "            means = gmm.means_.flatten()\n",
    "            covariances = np.sqrt(np.array([np.linalg.inv(gmm.precisions_[i]).flatten()[0] for i in range(2)]))\n",
    "            weights = gmm.weights_\n",
    "\n",
    "            # Calculer le seuil de séparation fast/slow\n",
    "            threshold = (means[0] + means[1]) / 2\n",
    "\n",
    "            # Prédire les labels pour chaque point\n",
    "            labels = np.where(log_data < threshold, 0, 1)\n",
    "\n",
    "            # Calculer les pourcentages\n",
    "            slow_percentage = np.sum(labels == 0) / len(labels) * 100\n",
    "            fast_percentage = np.sum(labels == 1) / len(labels) * 100\n",
    "\n",
    "            # Enregistrer les paramètres\n",
    "            f.write(f\"Colonne analysée : {column_name}\\n\")\n",
    "            f.write(f\"Pourcentage de molécules lentes (slow) : {slow_percentage:.2f}%\\n\")\n",
    "            f.write(f\"Pourcentage de molécules rapides (fast) : {fast_percentage:.2f}%\\n\")\n",
    "            f.write(f\"Moyennes des composantes GMM : {means}\\n\")\n",
    "            f.write(f\"Écarts-types des composantes GMM : {covariances}\\n\")\n",
    "            f.write(f\"Seuil de séparation fast/slow : {threshold}\\n\")\n",
    "\n",
    "            # Tracer les histogrammes et les densités\n",
    "            sns.histplot(log_data, bins=30, kde=False, color='gray', label=f'Données {column_name}', stat='density')\n",
    "\n",
    "            # Calculer les densités de probabilité pour les composantes\n",
    "            x_vals = np.linspace(-4, 0, 1000)\n",
    "\n",
    "            # Densités pour les composantes lente et rapide\n",
    "            pdf_0 = norm.pdf(x_vals, means[0], covariances[0])\n",
    "            pdf_1 = norm.pdf(x_vals, means[1], covariances[1])\n",
    "\n",
    "            # Tracer les composantes pondérées\n",
    "            plt.plot(x_vals, weights[0] * pdf_0, label='Composante Lente (Slow)', color='green')\n",
    "            plt.plot(x_vals, weights[1] * pdf_1, label='Composante Rapide (Fast)', color='orange')\n",
    "\n",
    "            # Densité totale du GMM\n",
    "            pdf_total = weights[0] * pdf_0 + weights[1] * pdf_1\n",
    "            plt.plot(x_vals, pdf_total, label='Ajustement GMM', color='red')\n",
    "\n",
    "            # Calcul de l'aire sous la courbe\n",
    "            area_slow = simps(weights[0] * pdf_0, x_vals)\n",
    "            area_fast = simps(weights[1] * pdf_1, x_vals)\n",
    "            area_total = simps(pdf_total, x_vals)\n",
    "\n",
    "            # Calcul du R² pour mesurer l'ajustement\n",
    "            r2 = r2_score(log_data, np.interp(log_data, x_vals, pdf_total))\n",
    "\n",
    "            # Enregistrer l'ajustement et R² dans le fichier texte\n",
    "            f.write(f\"Aire sous la courbe Slow : {area_slow:.4f}\\n\")\n",
    "            f.write(f\"Aire sous la courbe Fast : {area_fast:.4f}\\n\")\n",
    "            f.write(f\"Aire totale : {area_total:.4f}\\n\")\n",
    "            f.write(f\"R² de l'ajustement : {r2:.4f}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "            # Ajouter le seuil de séparation sur le graphique\n",
    "            plt.axvline(x=threshold, color='blue', linestyle='--', label='Seuil Fast/Slow')\n",
    "            plt.legend()\n",
    "\n",
    "            # Sauvegarder la figure\n",
    "            figure_name = f'gmm_{column_name}_{os.path.basename(file_path)}_{sheet_name}.png'\n",
    "            plt.savefig(figure_name)\n",
    "            plt.close()\n",
    "\n",
    "        # Appliquer l'analyse pour les colonnes RANK et RANKL\n",
    "        for col in ['Diff RANK', 'Diff RANKL']:\n",
    "            if col in data.columns:\n",
    "                analyze_and_plot(col)\n",
    "\n",
    "print(f\"Les résultats ont été sauvegardés et les figures enregistrées.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81bcd3-befe-4c68-9e80-6faf7df53e3d",
   "metadata": {},
   "source": [
    "Code qui permet de récupérer l'ensemble des fichiers textes pour faire un tableau comportant les pourcentages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb5537-1be8-44d9-bddf-0af90f10fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bon code \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Fonction pour traiter un fichier texte GMM\n",
    "def process_gmm_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Extraire les données pour D RANKL et D RANK\n",
    "    data = {}\n",
    "    current_col = None\n",
    "    for line in lines:\n",
    "        if \"Colonne analysée\" in line:\n",
    "            current_col = line.strip().split(':')[-1].strip()\n",
    "            data[current_col] = {}\n",
    "        elif \"Pourcentage de molécules lentes\" in line:\n",
    "            data[current_col][\"Pourcentage de molécules lentes (slow)\"] = line.split(':')[-1].strip()\n",
    "        elif \"Pourcentage de molécules rapides\" in line:\n",
    "            data[current_col][\"Pourcentage de molécules rapides (fast)\"] = line.split(':')[-1].strip()\n",
    "        elif \"Moyennes des composantes GMM\" in line:\n",
    "            data[current_col][\"Moyennes des composantes GMM\"] = line.split(':')[-1].strip()\n",
    "        elif \"Écarts-types des composantes GMM\" in line:\n",
    "            data[current_col][\"Écarts-types des composantes GMM\"] = line.split(':')[-1].strip()\n",
    "        elif \"Seuil de séparation fast/slow\" in line:\n",
    "            data[current_col][\"Seuil de séparation fast/slow\"] = line.split(':')[-1].strip()\n",
    "        elif \"Aire sous la courbe de la composante Lente\" in line:\n",
    "            data[current_col][\"Aire sous la courbe de la composante Lente (Slow)\"] = line.split(':')[-1].strip()\n",
    "        elif \"Aire sous la courbe de la composante Rapide\" in line:\n",
    "            data[current_col][\"Aire sous la courbe de la composante Rapide (Fast)\"] = line.split(':')[-1].strip()\n",
    "        elif \"Aire sous la courbe de l'ajustement GMM total\" in line:\n",
    "            data[current_col][\"Aire sous la courbe de l'ajustement GMM total\"] = line.split(':')[-1].strip()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Fonction principale pour traiter tous les fichiers texte dans un dossier\n",
    "def process_all_gmm_files_in_folder(folder_path):\n",
    "    # Créer des listes pour stocker les données pour D RANKL et D RANK\n",
    "    rankl_data = []\n",
    "    rank_data = []\n",
    "\n",
    "    # Colonnes pour le fichier Excel\n",
    "    columns = [\n",
    "        \"Nom du fichier\", \n",
    "        \"Pourcentage de molécules lentes (slow)\", \n",
    "        \"Pourcentage de molécules rapides (fast)\", \n",
    "        \"Moyennes des composantes GMM\", \n",
    "        \"Écarts-types des composantes GMM\", \n",
    "        \"Seuil de séparation fast/slow\", \n",
    "        \"Aire sous la courbe de la composante Lente (Slow)\", \n",
    "        \"Aire sous la courbe de la composante Rapide (Fast)\", \n",
    "        \"Aire sous la courbe de l'ajustement GMM total\"\n",
    "    ]\n",
    "\n",
    "    # Parcourir tous les fichiers .txt dans le dossier\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Traiter le fichier texte et extraire les données\n",
    "            gmm_data = process_gmm_txt(file_path)\n",
    "            \n",
    "            # Ajouter les données pour D RANKL et D RANK dans leurs listes respectives\n",
    "            if \"Diff RANKL\" in gmm_data:\n",
    "                rankl_data.append({\n",
    "                    \"Nom du fichier\": filename,\n",
    "                    **gmm_data[\"Diff RANKL\"]\n",
    "                })\n",
    "            if \"Diff RANK\" in gmm_data:\n",
    "                rank_data.append({\n",
    "                    \"Nom du fichier\": filename,\n",
    "                    **gmm_data[\"Diff RANK\"]\n",
    "                })\n",
    "\n",
    "    # Créer des DataFrames pour D RANKL et D RANK\n",
    "    df_rankl = pd.DataFrame(rankl_data, columns=columns)\n",
    "    df_rank = pd.DataFrame(rank_data, columns=columns)\n",
    "\n",
    "    # Enregistrer les données dans un fichier Excel avec deux feuilles\n",
    "    output_excel_path = os.path.join(folder_path, 'gmm_combined_output.xlsx')\n",
    "    with pd.ExcelWriter(output_excel_path) as writer:\n",
    "        df_rankl.to_excel(writer, sheet_name='Diff RANKL', index=False)\n",
    "        df_rank.to_excel(writer, sheet_name='Diff RANK', index=False)\n",
    "    \n",
    "    print(f\"Le fichier Excel a été enregistré sous : {output_excel_path}\")\n",
    "\n",
    "# Spécifier le dossier contenant les fichiers .txt\n",
    "folder_path = '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier test longtrajgamma'  # Remplacez par le chemin du dossier où sont stockés vos fichiers .txt\n",
    "\n",
    "# Exécuter la fonction pour traiter tous les fichiers dans le dossier\n",
    "process_all_gmm_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312de17-d8e2-4946-b016-785959f7b0ba",
   "metadata": {},
   "source": [
    "Récupérer les gamma dans combined_output_202* avec valeurs fixe des seuils calcul des % des différentes populations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6591f-72c2-4c67-9611-7d4138ed0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour plusieurs dossiers excels \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Définir les seuils\n",
    "CONFINE_LIMIT = 0.75\n",
    "BROWNIAN_LIMIT = 1.25\n",
    "\n",
    "# Fonction pour analyser les données et générer les fichiers Excel consolidés\n",
    "def analyze_multiple_excel_files(file_paths, output_folder):\n",
    "    # Initialiser des dictionnaires pour stocker les DataFrames par fichier d'entrée\n",
    "    rank_sheets = {}\n",
    "    rankl_sheets = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Charger le fichier Excel\n",
    "        excel_data = pd.ExcelFile(file_path)\n",
    "        rank_results = []\n",
    "        rankl_results = []\n",
    "\n",
    "        # Parcourir chaque feuille\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            df = excel_data.parse(sheet_name)\n",
    "\n",
    "            for column, results in [(\"gamma RANK\", rank_results), (\"gamma RANKL\", rankl_results)]:\n",
    "                if column in df.columns:\n",
    "                    # Extraire et filtrer les valeurs\n",
    "                    gamma_values = df[column].dropna()\n",
    "                    #gamma_values = gamma_values[(gamma_values > 0) & (gamma_values < 3)]\n",
    "                    gamma_values = gamma_values[gamma_values != 0] # exlu uniquement les 0\n",
    "\n",
    "                    if len(gamma_values) == 0:\n",
    "                        continue  # Passer si aucune valeur valide\n",
    "\n",
    "                    # Calculer les pourcentages\n",
    "                    total_values = len(gamma_values)\n",
    "                    confined_count = len(gamma_values[gamma_values < CONFINE_LIMIT])\n",
    "                    brownian_count = len(gamma_values[(gamma_values >= CONFINE_LIMIT) & (gamma_values < BROWNIAN_LIMIT)])\n",
    "                    linear_count = len(gamma_values[gamma_values >= BROWNIAN_LIMIT])\n",
    "\n",
    "                    confined_percent = (confined_count / total_values) * 100\n",
    "                    brownian_percent = (brownian_count / total_values) * 100\n",
    "                    linear_percent = (linear_count / total_values) * 100\n",
    "\n",
    "                    # Ajouter les résultats à la liste\n",
    "                    results.append({\n",
    "                        \"Nom du fichier\": os.path.basename(file_path),\n",
    "                        \"Nom de la feuille\": sheet_name,\n",
    "                        \"% Confiné\": confined_percent,\n",
    "                        \"% Brownien\": brownian_percent,\n",
    "                        \"% Linéaire\": linear_percent\n",
    "                    })\n",
    "\n",
    "        # Ajouter les résultats pour ce fichier à un DataFrame\n",
    "        rank_sheets[os.path.basename(file_path)] = pd.DataFrame(rank_results)\n",
    "        rankl_sheets[os.path.basename(file_path)] = pd.DataFrame(rankl_results)\n",
    "\n",
    "    # Créer des fichiers Excel consolidés avec plusieurs feuilles\n",
    "    rank_output_file = os.path.join(output_folder, \"gamma_RANK_percentages.xlsx\")\n",
    "    rankl_output_file = os.path.join(output_folder, \"gamma_RANKL_percentages.xlsx\")\n",
    "\n",
    "    with pd.ExcelWriter(rank_output_file) as writer:\n",
    "        for file_name, df in rank_sheets.items():\n",
    "            df.to_excel(writer, sheet_name=file_name[:31], index=False)  # Limiter le nom de la feuille à 31 caractères\n",
    "\n",
    "    with pd.ExcelWriter(rankl_output_file) as writer:\n",
    "        for file_name, df in rankl_sheets.items():\n",
    "            df.to_excel(writer, sheet_name=file_name[:31], index=False)\n",
    "\n",
    "    print(f\"Fichiers Excel consolidés générés : {rank_output_file}, {rankl_output_file}\")\n",
    "\n",
    "# Liste des fichiers Excel à analyser\n",
    "file_paths = [\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2023-08-10_+Ac.xlsx',\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2023-08-10_Control.xlsx',\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2024-02-13_+Ac.xlsx',\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2024-02-13_Control.xlsx',\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2024-02-14_+Ac.xlsx',\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2024-02-14_Control.xlsx',\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2024-02-29_+Ac.xlsx',\n",
    "    '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac/combined_output_2024-02-29_Control.xlsx',\n",
    " ]\n",
    "\n",
    "# Dossier de sortie\n",
    "output_folder = '/Users/lorie/Library/CloudStorage/GoogleDrive-maillot.loriane@gmail.com/Mon Drive/thèse/Résultats/microscopie/Résultats laboratoire/Code matalab et python Loriane/Gaussian/dossier relance gamma D traj long/+Ac'\n",
    "\n",
    "# Exécuter la fonction\n",
    "analyze_multiple_excel_files(file_paths, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
